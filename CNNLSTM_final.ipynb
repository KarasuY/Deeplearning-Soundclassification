{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTM_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcui5/dl-final/blob/main/CNNLSTM_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKGmJuLeUQ7d",
        "outputId": "f59ec97d-6176-4b1e-b133-c1870413debb"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "import pickle \n",
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "from google.colab import drive #Can ignore if done locally\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it-lJvapclBF"
      },
      "source": [
        "def preprocess(filepath): \n",
        "  \"\"\"\n",
        "    1. Unpickle file\n",
        "    2. Separate \n",
        "    3. One-hot encode labels \n",
        "\n",
        "    :inputs: \n",
        "    filepath: filepath to the pickle file in Drive \n",
        "\n",
        "    :returns: \n",
        "    (inputs, labels, folders)\n",
        "  \"\"\"\n",
        "  \n",
        "  with open(filepath, 'rb') as fo:\n",
        "    pickle_output = pickle.load(fo, encoding='bytes')\n",
        "  \n",
        "  inputs = [row[0] for row in pickle_output]\n",
        "  inputs = [inputs[i][:173] for i in range(len(inputs))]\n",
        "  inputs = np.array(inputs)\n",
        "  labels = np.array(pickle_output)[:, 1]\n",
        "  folders = np.array(pickle_output)[:, 2]\n",
        "\n",
        "  return (inputs, labels, folders)\n",
        "\n",
        "def split(inputs, labels, folders, test_folder_idx):\n",
        "  \"\"\"\n",
        "    Split data into training and testing data \n",
        "\n",
        "    :inputs: \n",
        "    the outputs from preprocess \n",
        "    test_folder_idx: index of the folder that will be used for testing\n",
        "\n",
        "    :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  \"\"\"\n",
        "  test_indices = np.nonzero(folders == test_folder_idx)\n",
        "  train_indices = np.nonzero(folders != test_folder_idx)\n",
        "\n",
        "  return (inputs[train_indices], labels[train_indices], inputs[test_indices], labels[test_indices])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOWn7o16VYN"
      },
      "source": [
        "def shuffle(inputs, labels, test_fraction):\n",
        "  '''\n",
        "  shuffle collection of all data, and split into testing and training, 15%:85%\n",
        "\n",
        "  :inputs: \n",
        "    the outputs from preprocess (inputs and labels)\n",
        "    test_fraction: percentage of inputs that will be used for testing\n",
        "  \n",
        "  :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  '''\n",
        "  indices = np.arange(labels.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  inputs = np.take(inputs, indices, axis=0)\n",
        "  labels = np.take(labels, indices, axis=0)\n",
        "\n",
        "  num_test = int(test_fraction * labels.shape[0])\n",
        "  test_inputs = inputs[:num_test]\n",
        "  test_labels = labels[:num_test]\n",
        "  train_inputs = inputs[num_test:]\n",
        "  train_labels = labels[num_test:]\n",
        "\n",
        "  return (train_inputs, train_labels, test_inputs, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x8QpbkXT-dF"
      },
      "source": [
        "class CNNLSTMModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_batches):\n",
        "    super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "    self.lstm_dropout = 0.2\n",
        "    self.dropout_rate = 0.8 #0.25\n",
        "    self.lstm_size = 256\n",
        "    self.batch_size = num_batches\n",
        "\n",
        "    #adam optimizer\n",
        "    self.optimizer = tf.keras.optimizers.Adam(lr = 1e-4)\n",
        "\n",
        "    #initialize layers\n",
        "    self.lstm1 = tf.keras.layers.LSTM(self.lstm_size, dropout=self.lstm_dropout)\n",
        "    self.dense1 = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=4, kernel_size=(5,5), strides=(4,1), activation='relu')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,1), activation='relu')\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(2,2), strides=(2,1), activation='relu')\n",
        "   \n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters=300, kernel_size=(2,2), strides=(1,1), activation='relu')\n",
        "\n",
        "    self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "    self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    #padding='same' if this doesn't work\n",
        "\n",
        "\n",
        "    inputs = tf.expand_dims(inputs, axis=3)\n",
        "\n",
        "\n",
        "    convlayer1 = self.conv1(inputs)\n",
        "    # convlayer2 = self.conv2(convlayer1)\n",
        "    maxpool1 = self.maxpool1(convlayer1)\n",
        "\n",
        "    drop1 = self.dropout1(maxpool1)\n",
        "    convlayer3 = self.conv3(drop1)\n",
        "    maxpool2 = self.maxpool2(convlayer3)\n",
        "\n",
        "    drop2 = self.dropout2(maxpool2)\n",
        "    convlayer4 = self.conv4(drop2)\n",
        "    drop3 = self.dropout3(convlayer4)\n",
        "\n",
        "    # reshape = tf.reshape(drop3, (100,300,32))\n",
        "    reshape = tf.reshape(drop3, (100,300,102))\n",
        "\n",
        "    lstm = self.lstm1(reshape)\n",
        "    dense = self.dense1(lstm)\n",
        "\n",
        "    return dense\n",
        "\n",
        "  def loss(self, logits, labels):\n",
        "\n",
        "    '''As cited in the paper, Table IV Experimental Results, authors used Categorical\n",
        "      cross entropy loss to measure their CNN+LSTM models'''\n",
        "    #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, logits))\n",
        "    losses = tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    return tf.reduce_mean(losses)\n",
        "\n",
        "  def accuracy(self, logits, labels):\n",
        "\n",
        "    correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(labels,1))\n",
        "    return tf.reduce_sum(tf.cast(correct_predictions, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KCageXMUgCz"
      },
      "source": [
        "def train(model, train_inputs, train_labels):\n",
        "  for i in range(len(train_inputs) // model.batch_size):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = train_inputs[start : start + model.batch_size]\n",
        "      labels = train_labels[start : start + model.batch_size]\n",
        "\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # forward pass \n",
        "          logits = model.call(inputs)\n",
        "          loss = model.loss(logits, labels)\n",
        "                  \n",
        "      # backprop \n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "def test(model, test_inputs, test_labels): \n",
        "  \"\"\"\n",
        "    returns the TOTAL accuracy for a single FOLDER \n",
        "  \"\"\"\n",
        "  num_batches = len(test_inputs) // model.batch_size\n",
        "  total_right = 0\n",
        "\n",
        "  for i in range(num_batches):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = test_inputs[start : start + model.batch_size]\n",
        "      labels = test_labels[start : start + model.batch_size]\n",
        "\n",
        "      # calling the model to get our probabilities \n",
        "      logits = model.call(inputs)\n",
        "      total_right += model.accuracy(logits, labels)\n",
        "  \n",
        "  return total_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_weaLzcuHT"
      },
      "source": [
        "# pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "# inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "# batch_size, num_epochs = 100, 250\n",
        "  \n",
        "# accuracy = 0 \n",
        "# total_tested = 0 \n",
        "# for i in range(10):\n",
        "#   print(\"Split/test folder: \", i + 1) \n",
        "#   model = CNNLSTMModel(batch_size)\n",
        "#   tr_in, tr_lb, te_in, te_lb = split(inputs, labels, folders, i + 1)\n",
        "#   tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "#   te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "#   tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "#   te_lb = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "#   for i in range(num_epochs): \n",
        "#     train(model, tr_in, tr_lb)\n",
        "#     print(i)\n",
        "\n",
        "#   per_fold_acc = test(model, te_in, te_lb)\n",
        "#   accuracy += per_fold_acc\n",
        "#   print('Cumulative right: ' + str(accuracy))\n",
        "     \n",
        "#   per_fold_tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "#   total_tested += per_fold_tested\n",
        "#   print('per-fold acc: ' + str(per_fold_acc / per_fold_tested))\n",
        "  \n",
        "# print(\"Total Average Accuracy: \", accuracy / total_tested)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G34do04U6kFA",
        "outputId": "5b2256b8-9224-4d6a-9862-ef0c1a64a885"
      },
      "source": [
        "'''\n",
        "Shuffled version: extracted 15% for testing (follows paper)\n",
        "'''\n",
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 100, 500\n",
        "  \n",
        "model = CNNLSTMModel(batch_size)\n",
        "print(np.shape(inputs))\n",
        "tr_in, tr_lb, te_in, te_lb = shuffle(inputs, labels, 0.15)\n",
        "\n",
        "tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "te_lb = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "for i in range(num_epochs): \n",
        "  train(model, tr_in, tr_lb)\n",
        "  print(i)\n",
        "acc = test(model, te_in, te_lb)\n",
        "print(np.shape(te_in))\n",
        "print(np.shape(te_lb))\n",
        "print(\"Average Accuracy: \", acc / te_lb.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6573, 173, 40)\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "(985, 173, 40)\n",
            "(985, 10)\n",
            "Average Accuracy:  tf.Tensor(0.7908629, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}