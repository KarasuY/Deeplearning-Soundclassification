{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcui5/dl-final/blob/main/LSTM_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oBopOkPf6AI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "107058ae-5568-46e2-9ba2-cd41a7789176"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "import pickle \n",
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw2QQdEAgJTe"
      },
      "source": [
        "def preprocess(filepath): \n",
        "  \"\"\"\n",
        "    1. Unpickle file\n",
        "    2. Separate \n",
        "    3. One-hot encode labels \n",
        "\n",
        "    :inputs: \n",
        "    filepath: filepath to the pickle file in Drive \n",
        "\n",
        "    :returns: \n",
        "    (inputs, labels, folders)\n",
        "  \"\"\"\n",
        "  \n",
        "  with open(filepath, 'rb') as fo:\n",
        "    pickle_output = pickle.load(fo, encoding='bytes')\n",
        "  \n",
        "  inputs = [row[0] for row in pickle_output]\n",
        "  inputs = [inputs[i][:173] for i in range(len(inputs))]\n",
        "  inputs = np.array(inputs)\n",
        "  labels = np.array(pickle_output)[:, 1]\n",
        "  folders = np.array(pickle_output)[:, 2]\n",
        "\n",
        "  return (inputs, labels, folders)\n",
        "\n",
        "def split(inputs, labels, folders, test_folder_idx):\n",
        "  \"\"\"\n",
        "    Split data into training and testing data \n",
        "\n",
        "    :inputs: \n",
        "    the outputs from preprocess \n",
        "    test_folder_idx: index of the folder that will be used for testing\n",
        "\n",
        "    :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  \"\"\"\n",
        "  test_indices = np.nonzero(folders == test_folder_idx)\n",
        "  train_indices = np.nonzero(folders != test_folder_idx)\n",
        "\n",
        "  return (inputs[train_indices], labels[train_indices], inputs[test_indices], labels[test_indices])\n",
        "\n",
        "def shuffle(inputs, labels, test_fraction):\n",
        "  '''\n",
        "  shuffle collection of all data, and split into testing and training, 15%:85%\n",
        "\n",
        "  :inputs: \n",
        "    the outputs from preprocess (inputs and labels)\n",
        "    test_fraction: percentage of inputs that will be used for testing\n",
        "  \n",
        "  :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  '''\n",
        "  indices = np.arange(labels.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  inputs = np.take(inputs, indices, axis=0)\n",
        "  labels = np.take(labels, indices, axis=0)\n",
        "\n",
        "  num_test = int(test_fraction * labels.shape[0])\n",
        "  test_inputs = inputs[:num_test]\n",
        "  test_labels = labels[:num_test]\n",
        "  train_inputs = inputs[num_test:]\n",
        "  train_labels = labels[num_test:]\n",
        "\n",
        "  return (train_inputs, train_labels, test_inputs, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifKZsLTbatsI"
      },
      "source": [
        "class LSTM3(tf.keras.Model):\n",
        "  \"\"\"\n",
        "    Model based on LSTM3 in Table II of the paper\n",
        "  \"\"\"\n",
        "  def __init__(self, batch_size): \n",
        "\n",
        "    super(LSTM3, self).__init__()\n",
        "\n",
        "    self.num_classes = 10 \n",
        "    self.lstm1_units = 256\n",
        "    self.lstm2_units = 128\n",
        "    self.lstm3_units = 64 \n",
        "    self.dropout_rate = 0.2 # Paper default is 0.2\n",
        "    self.dense_size = 10 \n",
        "\n",
        "    self.learning_rate = 1e-4\n",
        "    self.batch_size = batch_size \n",
        "\n",
        "    self.lstm1_layer = tf.keras.layers.LSTM(self.lstm1_units, return_sequences=True, dropout=self.dropout_rate)\n",
        "    self.lstm2_layer = tf.keras.layers.LSTM(self.lstm2_units, return_sequences=True, dropout=self.dropout_rate)\n",
        "    self.lstm3_layer = tf.keras.layers.LSTM(self.lstm3_units, dropout=self.dropout_rate)\n",
        "\n",
        "    self.dense_layer = tf.keras.layers.Dense(self.num_classes, activation='softmax')\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "  def call(self, inputs): \n",
        "    layer1_output = self.lstm1_layer(inputs, None)\n",
        "    layer2_output = self.lstm2_layer(layer1_output, None)\n",
        "    layer3_output = self.lstm3_layer(layer2_output, None) \n",
        "    probabilities = self.dense_layer(layer3_output) \n",
        "\n",
        "    return probabilities \n",
        "\n",
        "  def loss(self, probabilities, labels):\n",
        "    losses = tf.keras.losses.categorical_crossentropy(labels, probabilities, from_logits=False)\n",
        "    return tf.reduce_mean(losses)\n",
        "\n",
        "  def accuracy(self, probabilities, labels):\n",
        "    \"\"\"\n",
        "      returns TOTAL NUMBER correct over a batch (does not average)\n",
        "    \"\"\" \n",
        "    correct_predictions = tf.equal(tf.argmax(probabilities, 1), tf.argmax(labels, 1))    \n",
        "    return tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
        "  \n",
        "  def accuracy_2(self, probabilities, labels): \n",
        "    \"\"\"\n",
        "      returns the TOTAL NUMBER correct for our binary classification \n",
        "      (dangerous versus non-dangerous sounds)\n",
        "    \"\"\"\n",
        "    # dangerous labels include car_horn, dog_bark, drilling, gun_shot, jackhammer, siren\n",
        "    DANGEROUS_LABELS = [1, 3, 4, 6, 7, 8]  \n",
        "\n",
        "    # get the correct classification\n",
        "    classified = tf.argmax(probabilities, 1)\n",
        "    # for each classification, classify it as dangerous or not dangerous \n",
        "    classified_binary = tf.map_fn(lambda x: x in DANGEROUS_LABELS, classified)\n",
        "    \n",
        "    # get the label, and for each label classify it as dangerous or not dangerous\n",
        "    labels_classes = tf.argmax(labels, 1)\n",
        "    labels_binary = tf.map_fn(lambda x: x in DANGEROUS_LABELS, labels_classes)\n",
        "\n",
        "    # count the overlap and return the number correct in the given batch \n",
        "    correct_predictions = tf.equal(classified_binary, labels_binary)\n",
        "    return tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "\n",
        "def train(model, train_inputs, train_labels):\n",
        "  \"\"\"\n",
        "    trains model by batching train_inputs and updates weights based on loss\n",
        "  \"\"\"\n",
        "  for i in range(len(train_inputs) // model.batch_size):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = train_inputs[start : start + model.batch_size]\n",
        "      labels = train_labels[start : start + model.batch_size]\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # forward pass \n",
        "          probabilities = model.call(inputs)\n",
        "          loss = model.loss(probabilities, labels)\n",
        "                  \n",
        "      # backprop \n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "def test(model, test_inputs, test_labels, binary=False): \n",
        "  \"\"\"\n",
        "    returns the TOTAL accuracy for a single FOLDER \n",
        "  \"\"\"\n",
        "  num_batches = len(test_inputs) // model.batch_size\n",
        "  total_right = 0\n",
        "\n",
        "  for i in range(num_batches):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = test_inputs[start : start + model.batch_size]\n",
        "      labels = test_labels[start : start + model.batch_size]\n",
        "\n",
        "      # calling the model to get our probabilities \n",
        "      probabilities = model.call(inputs)\n",
        "      if binary: \n",
        "        total_right += model.accuracy_2(probabilities, labels)\n",
        "      else: \n",
        "        total_right += model.accuracy(probabilities, labels)\n",
        "  \n",
        "  return total_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCogiU7VsEvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9e49b5-c88f-4752-b945-feb271a805d1"
      },
      "source": [
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 50, 500\n",
        "  \n",
        "accuracy, binary_accuracy = 0, 0 \n",
        "total_tested = 0 \n",
        "for i in range(10): # change this back to 10 when everything is done testing \n",
        "  print(\"Split/test folder: \", i + 1) \n",
        "  model = LSTM3(batch_size)\n",
        "  tr_in, tr_lb, te_in, te_lb = split(inputs, labels, folders, i + 1)\n",
        "  tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "  te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "  tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "  te_lb = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "\n",
        "  for _ in range(num_epochs): \n",
        "    train(model, tr_in, tr_lb)\n",
        "\n",
        "  per_fold_acc, binary_fold_acc = test(model, te_in, te_lb), test(model, te_in, te_lb, True)\n",
        "  accuracy += per_fold_acc\n",
        "  binary_accuracy += binary_fold_acc\n",
        "     \n",
        "  per_fold_tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "  total_tested += per_fold_tested\n",
        "  print('per-fold acc (10-class): ' + str(per_fold_acc / per_fold_tested))\n",
        "  print('per-fold acc (BINARY):   ' + str(binary_fold_acc / per_fold_tested))\n",
        "  \n",
        "print(\"Total Average Accuracy (10-class): \", accuracy / total_tested)\n",
        "print(\"Total Average Accuracy (BINARY):   \", binary_accuracy / total_tested)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split/test folder:  1\n",
            "per-fold acc (10-class): tf.Tensor(0.48307693, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.8261539, shape=(), dtype=float32)\n",
            "Split/test folder:  2\n",
            "per-fold acc (10-class): tf.Tensor(0.5030769, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.7876923, shape=(), dtype=float32)\n",
            "Split/test folder:  3\n",
            "per-fold acc (10-class): tf.Tensor(0.4757143, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.63714284, shape=(), dtype=float32)\n",
            "Split/test folder:  4\n",
            "per-fold acc (10-class): tf.Tensor(0.56857145, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.73142856, shape=(), dtype=float32)\n",
            "Split/test folder:  5\n",
            "per-fold acc (10-class): tf.Tensor(0.6742857, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.8642857, shape=(), dtype=float32)\n",
            "Split/test folder:  6\n",
            "per-fold acc (10-class): tf.Tensor(0.505, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.805, shape=(), dtype=float32)\n",
            "Split/test folder:  7\n",
            "per-fold acc (10-class): tf.Tensor(0.62333333, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.82666665, shape=(), dtype=float32)\n",
            "Split/test folder:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plVLmPBnyiWl",
        "outputId": "bbafb459-e4cd-4e01-eb5c-9f5ed9ee61fa"
      },
      "source": [
        "'''\n",
        "Shuffled version: extracted 15% for testing (follows paper) - Inflated scores\n",
        "'''\n",
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 50, 250\n",
        "\n",
        "model = LSTM3(batch_size)\n",
        "print(np.shape(inputs))\n",
        "tr_in, tr_lb, te_in, te_lb = shuffle(inputs, labels, 0.15)\n",
        "\n",
        "tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "te_lb = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "for _ in range(num_epochs): \n",
        "  train(model, tr_in, tr_lb)\n",
        "\n",
        "acc, binary_acc = test(model, te_in, te_lb), test(model, te_in, te_lb, True)\n",
        "print(np.shape(te_in))\n",
        "print(np.shape(te_lb))\n",
        "tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "print(\"10 class Accuracy: \", acc / tested)\n",
        "print(\"BINARY Accuracy: \", binary_acc / tested)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6573, 173, 40)\n",
            "(985, 173, 40)\n",
            "(985, 10)\n",
            "10 class Accuracy:  tf.Tensor(0.91473687, shape=(), dtype=float32)\n",
            "BINARY Accuracy:  tf.Tensor(0.9631579, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}