{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcui5/dl-final/blob/main/Ensemble_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeu5nPYxGk1Z",
        "outputId": "77c92de7-98a8-4d95-b399-d44ae1d3ca30"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "import pickle \n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aNXVD1x6M_H"
      },
      "source": [
        "'''\n",
        "Functions for preprocessing and splitting/shuffling the data for the different\n",
        "measures of accuracy \n",
        "'''\n",
        "def preprocess(filepath): \n",
        "  \"\"\"\n",
        "    1. Unpickle file\n",
        "    2. Separate \n",
        "    3. One-hot encode labels \n",
        "\n",
        "    :inputs: \n",
        "    filepath: filepath to the pickle file in Drive \n",
        "\n",
        "    :returns: \n",
        "    (inputs, labels, folders)\n",
        "  \"\"\"\n",
        "  \n",
        "  with open(filepath, 'rb') as fo:\n",
        "    pickle_output = pickle.load(fo, encoding='bytes')\n",
        "  \n",
        "  inputs = [row[0] for row in pickle_output]\n",
        "  inputs = [inputs[i][:173] for i in range(len(inputs))]\n",
        "  inputs = np.array(inputs)\n",
        "  labels = np.array(pickle_output)[:, 1]\n",
        "  folders = np.array(pickle_output)[:, 2]\n",
        "\n",
        "  return (inputs, labels, folders)\n",
        "\n",
        "'''\n",
        "Moved to LSTM_3 file\n",
        "'''\n",
        "def split(inputs, labels, folders, test_folder_idx):\n",
        "  \"\"\"\n",
        "    Split data into training and testing data \n",
        "\n",
        "    :inputs: \n",
        "    the outputs from preprocess \n",
        "    test_folder_idx: index of the folder that will be used for testing\n",
        "\n",
        "    :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  \"\"\"\n",
        "  test_indices = np.nonzero(folders == test_folder_idx)\n",
        "  train_indices = np.nonzero(folders != test_folder_idx)\n",
        "\n",
        "  return (inputs[train_indices], labels[train_indices], inputs[test_indices], labels[test_indices])\n",
        "\n",
        "  '''\n",
        "Move into model files \n",
        "'''\n",
        "def shuffle(inputs, labels, test_fraction):\n",
        "  '''\n",
        "  shuffle collection of all data, and split into testing and training, 15%:85%\n",
        "\n",
        "  :inputs: \n",
        "    the outputs from preprocess (inputs and labels)\n",
        "    test_fraction: percentage of inputs that will be used for testing\n",
        "  \n",
        "  :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  '''\n",
        "  indices = np.arange(labels.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  inputs = np.take(inputs, indices, axis=0)\n",
        "  labels = np.take(labels, indices, axis=0)\n",
        "\n",
        "  num_test = int(test_fraction * labels.shape[0])\n",
        "  test_inputs = inputs[:num_test]\n",
        "  test_labels = labels[:num_test]\n",
        "  train_inputs = inputs[num_test:]\n",
        "  train_labels = labels[num_test:]\n",
        "\n",
        "  return (train_inputs, train_labels, test_inputs, test_labels)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPgg0MfTLoTh"
      },
      "source": [
        "# LSTM3 MODEL \n",
        "class LSTM3(tf.keras.Model):\n",
        "  \"\"\"\n",
        "    Model based on LSTM3 in Table II of the paper\n",
        "  \"\"\"\n",
        "  def __init__(self, batch_size): \n",
        "\n",
        "    super(LSTM3, self).__init__()\n",
        "\n",
        "    self.num_classes = 10 \n",
        "    self.lstm1_units = 256\n",
        "    self.lstm2_units = 128\n",
        "    self.lstm3_units = 64 \n",
        "    self.dropout_rate = 0.2\n",
        "    self.dense_size = 10 \n",
        "\n",
        "    self.learning_rate = 1e-4\n",
        "    self.batch_size = batch_size \n",
        "\n",
        "    self.lstm1_layer = tf.keras.layers.LSTM(self.lstm1_units, return_sequences=True, dropout=self.dropout_rate)\n",
        "    self.lstm2_layer = tf.keras.layers.LSTM(self.lstm2_units, return_sequences=True, dropout=self.dropout_rate)\n",
        "    self.lstm3_layer = tf.keras.layers.LSTM(self.lstm3_units, dropout=self.dropout_rate)\n",
        "\n",
        "    self.dense_layer = tf.keras.layers.Dense(self.num_classes, activation='softmax')\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "  '''\n",
        "  Calls all layers in model\n",
        "  Function returns probabilities\n",
        "  '''\n",
        "  def call(self, inputs): \n",
        "    layer1_output = self.lstm1_layer(inputs, None)\n",
        "    layer2_output = self.lstm2_layer(layer1_output, None)\n",
        "    layer3_output = self.lstm3_layer(layer2_output, None) \n",
        "    probabilities = self.dense_layer(layer3_output) \n",
        "\n",
        "    return probabilities \n",
        "  \n",
        "  '''\n",
        "  Computes categorical cross-entropy loss (per the paper)\n",
        "  Returns the average loss of a batch\n",
        "  '''\n",
        "  def loss(self, probabilities, labels):\n",
        "    losses = tf.keras.losses.categorical_crossentropy(labels, probabilities)\n",
        "    return tf.reduce_mean(losses)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw90Tb4dL2hQ"
      },
      "source": [
        "\n",
        "class CNNLSTMModel(tf.keras.Model):\n",
        "  \"\"\"\n",
        "    Model based on CNN-LSTM in Table III of the paper\n",
        "  \"\"\"\n",
        "  def __init__(self, num_batches):\n",
        "    super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "    self.lstm_dropout = 0.2\n",
        "    self.dropout_rate = 0.25\n",
        "    self.lstm_size = 256\n",
        "    self.batch_size = num_batches\n",
        "\n",
        "    #adam optimizer\n",
        "    self.optimizer = tf.keras.optimizers.Adam(lr = 1e-4)\n",
        "\n",
        "    #initialize layers\n",
        "    self.lstm1 = tf.keras.layers.LSTM(self.lstm_size, dropout=self.lstm_dropout)\n",
        "    self.dense1 = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=4, kernel_size=(5,5), strides=(4,1), activation='relu')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,1), activation='relu')\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(2,2), strides=(2,1), activation='relu')\n",
        "   \n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters=300, kernel_size=(2,2), strides=(1,1), activation='relu')\n",
        "\n",
        "    self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "    self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "  \n",
        "  '''\n",
        "  Calls all layers in model\n",
        "  Function returns probabilities\n",
        "  '''\n",
        "  def call(self, inputs):\n",
        "    inputs = tf.expand_dims(inputs, axis=3)\n",
        "\n",
        "    convlayer1 = self.conv1(inputs)\n",
        "    maxpool1 = self.maxpool1(convlayer1)\n",
        "\n",
        "    drop1 = self.dropout1(maxpool1)\n",
        "    convlayer3 = self.conv3(drop1)\n",
        "    maxpool2 = self.maxpool2(convlayer3)\n",
        "\n",
        "    drop2 = self.dropout2(maxpool2)\n",
        "    convlayer4 = self.conv4(drop2)\n",
        "    drop3 = self.dropout3(convlayer4)\n",
        "\n",
        "    reshape = tf.reshape(drop3, (self.batch_size,300,-1))\n",
        "\n",
        "\n",
        "    lstm = self.lstm1(reshape)\n",
        "    dense = self.dense1(lstm)\n",
        "\n",
        "    return dense\n",
        "\n",
        "  def loss(self, probabilities, labels):\n",
        "    \"\"\"\n",
        "    Calculates average categorical cross entropy loss of the prediction\n",
        "\n",
        "    :param probabilities: a matrix of logits as a tensor\n",
        "    :param labels: matrix of labels containing the labels\n",
        "    :return: the loss of the model as a tensor of size 1\n",
        "\n",
        "    As cited in the paper, Table IV Experimental Results, authors used Categorical\n",
        "  cross entropy loss to measure their CNN+LSTM models\n",
        "    \"\"\"\n",
        "\n",
        "    losses = tf.keras.losses.categorical_crossentropy(labels, probabilities, from_logits=False)\n",
        "    return tf.reduce_mean(losses)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpLBPurpMOyq"
      },
      "source": [
        "# CNN MODEL\n",
        "class CNNModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_batches):\n",
        "    super(CNNModel, self).__init__()\n",
        "\n",
        "    self.dropout_rate = 0.25\n",
        "    self.batch_size = num_batches\n",
        "\n",
        "    #adam optimizer\n",
        "    self.optimizer = tf.keras.optimizers.Adam(lr = 1e-4)\n",
        "\n",
        "    #initialize layers\n",
        "    self.dense1 = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=4, kernel_size=(5,5), strides=(4,1), activation='relu')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,1), activation='relu')\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(2,2), strides=(2,1), activation='relu')\n",
        "   \n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters=300, kernel_size=(2,2), strides=(1,1), activation='relu')\n",
        "\n",
        "    self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "    self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "  \n",
        "  '''\n",
        "  Calls all layers in model\n",
        "  Function returns probabilities\n",
        "  '''\n",
        "  def call(self, inputs):\n",
        "    inputs = tf.expand_dims(inputs, axis=3)\n",
        "\n",
        "    convlayer1 = self.conv1(inputs)\n",
        "    convlayer2 = self.conv2(convlayer1)\n",
        "    maxpool1 = self.maxpool1(convlayer2)\n",
        "\n",
        "    drop1 = self.dropout1(maxpool1)\n",
        "    convlayer3 = self.conv3(drop1)\n",
        "    maxpool2 = self.maxpool2(convlayer3)\n",
        "\n",
        "    drop2 = self.dropout2(maxpool2)\n",
        "    convlayer4 = self.conv4(drop2)\n",
        "    drop3 = self.dropout3(convlayer4)\n",
        "    drop3 = tf.reshape(drop3, [self.batch_size, -1])\n",
        "    dense = self.dense1(drop3)\n",
        "\n",
        "    return dense\n",
        "\n",
        "  def loss(self, probabilities, labels):\n",
        "    \"\"\"\n",
        "    Calculates average cross entropy loss of the prediction\n",
        "\n",
        "    :param probabilities: a matrix of logits as a tensor\n",
        "    :param labels: matrix of labels containing the labels\n",
        "    :return: the loss of the model as a tensor of size 1\n",
        "\n",
        "    As cited in the paper, Table IV Experimental Results, authors used Categorical\n",
        "    cross entropy loss to measure their CNN+LSTM models\n",
        "    \"\"\"\n",
        "\n",
        "    losses = tf.keras.losses.categorical_crossentropy(labels, probabilities, from_logits=False)\n",
        "    return tf.reduce_mean(losses)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V8JwdTNM-B9"
      },
      "source": [
        "# GENERAL TRAIN FUNCTION FOR ALL THREE OF OUR MODELS \n",
        "def train(model, train_inputs, train_labels):\n",
        "  \"\"\"\n",
        "    trains model by batching train_inputs and updates weights based on loss\n",
        "  \"\"\"\n",
        "  for i in range(len(train_inputs) // model.batch_size):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = train_inputs[start : start + model.batch_size]\n",
        "      labels = train_labels[start : start + model.batch_size]\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # forward pass \n",
        "          logits = model.call(inputs)\n",
        "          loss = model.loss(logits, labels)\n",
        "                  \n",
        "      # backprop \n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZGTFfXkOP1s"
      },
      "source": [
        "def accuracy_prediction_integration(model, probabilities, oh_labels, noh_labels): \n",
        "  \"\"\"\n",
        "    :inputs: \n",
        "    probabilities (batch_sz, num_classes) -> (100, 10)\n",
        "    for the ten classes, \n",
        "    oh-labels for each batch -> (100, 10) (one hotted)\n",
        "    noh-labels for each batch -> (100) (indices)\n",
        "\n",
        "    :returns: \n",
        "    a list of 10 accuracies, 0-9 represent the accuracies for each class -> (10,)\n",
        "  \"\"\"\n",
        "  num_right_per_class = np.zeros(10)\n",
        "  class_predictions = tf.argmax(probabilities, 1) # 100 predictions\n",
        "  # for each of the actual labels, count the number of each class \n",
        "  number_per_class = tf.reduce_sum(oh_labels, axis=0) \n",
        "\n",
        "  for i in range(model.batch_size): \n",
        "    # check if predictions is same as label \n",
        "    correct_class = noh_labels[i]\n",
        "    if class_predictions[i] == correct_class: \n",
        "      num_right_per_class[correct_class] += 1 \n",
        "\n",
        "  broadcasted_acc= num_right_per_class / tf.reshape(number_per_class, (1, 10))\n",
        "  broadcasted_acc = tf.cast(broadcasted_acc, dtype=tf.float32)\n",
        "\n",
        "  # removed nans, which may occur if there has been no samples from specific class\n",
        "  # for a given batch\n",
        "  nans_removed = np.where(tf.math.is_nan(broadcasted_acc), tf.zeros(broadcasted_acc.shape, dtype=tf.float32), broadcasted_acc)\n",
        "  return nans_removed * probabilities # (100, 10)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXW5ATM8XdhH"
      },
      "source": [
        "def test(models, test_inputs, test_oh_labels, test_noh_labels, is_binary=False): \n",
        "  \"\"\"\n",
        "    returns NUMBER CORRECT PREDICTIONS for a single FOLDER \n",
        "  \"\"\"\n",
        "  DANGEROUS_CLASSES = tf.convert_to_tensor([1, 3, 4, 6, 7, 8], dtype=tf.int64)\n",
        "  NONDANGEROUS_CLASSES = tf.convert_to_tensor([0, 2, 5, 9], dtype=tf.int64)\n",
        "  BATCH_SIZE = models[0].batch_size\n",
        "  num_batches = len(test_inputs) // BATCH_SIZE\n",
        "  total_right = 0\n",
        "  num_right = 0 \n",
        "  for i in range(num_batches):\n",
        "      # getting the proper batch \n",
        "      start = i * BATCH_SIZE\n",
        "      inputs = test_inputs[start : start + BATCH_SIZE]\n",
        "      oh_labels = test_oh_labels[start : start + BATCH_SIZE]\n",
        "      noh_labels = test_noh_labels[start : start + BATCH_SIZE]\n",
        "\n",
        "      # calling the model to get our probabilities\n",
        "      ensemble_output = ensemble(models, inputs, oh_labels, noh_labels, is_binary)\n",
        "\n",
        "      if is_binary: \n",
        "        noh_labels = tf.map_fn(lambda x: x in DANGEROUS_CLASSES, noh_labels)\n",
        "\n",
        "      correct_predictions = tf.equal(ensemble_output, noh_labels)    \n",
        "      num_right += tf.reduce_sum(tf.cast(correct_predictions, tf.float32)) \n",
        "\n",
        "  return num_right "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "givnvF3JWWHV"
      },
      "source": [
        "def ensemble(models, inputs, oh_labels, noh_labels, is_binary=False): \n",
        "  \"\"\"\n",
        "    :inputs: \n",
        "      list of models to be included in the ensemble + the labels \n",
        "\n",
        "    :return: \n",
        "      Accuracy-Prediction Integration Method ensemble will return predictions for a batch \n",
        "  \"\"\"\n",
        "  if is_binary: \n",
        "    BATCH_SIZE, NUM_CLASSES = inputs.shape[0], 2\n",
        "  else: \n",
        "    BATCH_SIZE, NUM_CLASSES = inputs.shape[0], oh_labels.shape[1]\n",
        "\n",
        "  model_integration_products = np.zeros((BATCH_SIZE, NUM_CLASSES))\n",
        "\n",
        "  DANGEROUS_CLASSES = tf.convert_to_tensor([1, 3, 4, 6, 7, 8], dtype=tf.int64)\n",
        "  NONDANGEROUS_CLASSES = tf.convert_to_tensor([0, 2, 5, 9], dtype=tf.int64)\n",
        "\n",
        "  for model in models: \n",
        "    probabilities = model.call(inputs)\n",
        "    accuracy_by_class = accuracy_prediction_integration(model, probabilities, oh_labels, noh_labels)\n",
        "\n",
        "    if is_binary: \n",
        "      dangerous_accuracy = tf.reduce_sum(tf.gather(accuracy_by_class, DANGEROUS_CLASSES, axis=1), axis=1)\n",
        "      \n",
        "      nondangerous_accuracy = tf.reduce_sum(tf.gather(accuracy_by_class, NONDANGEROUS_CLASSES, axis=1), axis=1)\n",
        "      \n",
        "      combined = tf.stack([nondangerous_accuracy, dangerous_accuracy], axis=1)\n",
        "      model_integration_products += combined\n",
        "    else: \n",
        "      model_integration_products += accuracy_by_class\n",
        "      \n",
        "    return tf.argmax(model_integration_products, axis=1)   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_EMX0g15lC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1fb865-02f2-49d9-d091-ba7f1f7d2f1a"
      },
      "source": [
        "'''\n",
        "main function running all 3 models with 10-fold cross-validation\n",
        "'''\n",
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 50, 100\n",
        "\n",
        "accuracy, binary_accuracy = 0, 0 \n",
        "total_tested = 0 \n",
        "for i in range(10): \n",
        "  print(\"Split/test folder: \", i + 1) \n",
        "  models = [LSTM3(batch_size), CNNLSTMModel(batch_size), CNNModel(batch_size)] \n",
        "\n",
        "  tr_in, tr_lb, te_in, te_lb = split(inputs, labels, folders, i + 1)\n",
        "  tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "  te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "  te_lb_noh = tf.convert_to_tensor(te_lb, dtype=tf.int64)\n",
        "  tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "  te_lb_oh = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "  for model in models: \n",
        "    for _ in range(num_epochs): \n",
        "      train(model, tr_in, tr_lb)\n",
        "\n",
        "  per_fold_acc = test(models, te_in, te_lb_oh, te_lb_noh)\n",
        "  accuracy += per_fold_acc\n",
        "  binary_fold_acc = test(models, te_in, te_lb_oh, te_lb_noh, is_binary=True)\n",
        "  binary_accuracy += binary_fold_acc\n",
        "     \n",
        "  per_fold_tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "  total_tested += per_fold_tested\n",
        "  print('per-fold acc (10-class): ' + str(per_fold_acc / per_fold_tested))\n",
        "  print('per-fold acc (BINARY):   ' + str(binary_fold_acc / per_fold_tested))\n",
        "  \n",
        "print(\"Total Average Accuracy (10-class): \", accuracy / total_tested)\n",
        "print(\"Total Average Accuracy (BINARY):   \", binary_accuracy / total_tested)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split/test folder:  1\n",
            "per-fold acc (10-class): tf.Tensor(0.52615386, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.7784615, shape=(), dtype=float32)\n",
            "Split/test folder:  2\n",
            "per-fold acc (10-class): tf.Tensor(0.6476923, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.83692306, shape=(), dtype=float32)\n",
            "Split/test folder:  3\n",
            "per-fold acc (10-class): tf.Tensor(0.6, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.82, shape=(), dtype=float32)\n",
            "Split/test folder:  4\n",
            "per-fold acc (10-class): tf.Tensor(0.60571426, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.71428573, shape=(), dtype=float32)\n",
            "Split/test folder:  5\n",
            "per-fold acc (10-class): tf.Tensor(0.62142855, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.85571426, shape=(), dtype=float32)\n",
            "Split/test folder:  6\n",
            "per-fold acc (10-class): tf.Tensor(0.595, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.775, shape=(), dtype=float32)\n",
            "Split/test folder:  7\n",
            "per-fold acc (10-class): tf.Tensor(0.55333334, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.7583333, shape=(), dtype=float32)\n",
            "Split/test folder:  8\n",
            "per-fold acc (10-class): tf.Tensor(0.5163636, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.71272725, shape=(), dtype=float32)\n",
            "Split/test folder:  9\n",
            "per-fold acc (10-class): tf.Tensor(0.6566667, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.81333333, shape=(), dtype=float32)\n",
            "Split/test folder:  10\n",
            "per-fold acc (10-class): tf.Tensor(0.6116667, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.82, shape=(), dtype=float32)\n",
            "Total Average Accuracy (10-class):  tf.Tensor(0.5946457, shape=(), dtype=float32)\n",
            "Total Average Accuracy (BINARY):    tf.Tensor(0.7897638, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAaIkjtt80M7",
        "outputId": "e54de112-b719-42f1-96f7-c2bf52d66f00"
      },
      "source": [
        "'''\n",
        "Running shuffled computations for all 3 models:\n",
        "extracted 15% for testing (follows paper) - Inflated scores\n",
        "'''\n",
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 50, 50\n",
        "\n",
        "models = [LSTM3(batch_size), CNNLSTMModel(batch_size), CNNModel(batch_size)] \n",
        "print(np.shape(inputs))\n",
        "tr_in, tr_lb, te_in, te_lb = shuffle(inputs, labels, 0.15)\n",
        "\n",
        "tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "te_lb_noh = tf.convert_to_tensor(te_lb, dtype=tf.int64)\n",
        "tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "te_lb_oh = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "for model in models: \n",
        "    for _ in range(num_epochs): \n",
        "      train(model, tr_in, tr_lb)\n",
        "\n",
        "acc, binary_acc = test(models, te_in, te_lb_oh, te_lb_noh), test(models, te_in, te_lb_oh, te_lb_noh, is_binary=True)\n",
        "print(np.shape(te_in))\n",
        "print(np.shape(te_lb))\n",
        "tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "print(\"10 class Accuracy: \", acc / tested)\n",
        "print(\"BINARY Accuracy: \", binary_acc / tested)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6573, 173, 40)\n",
            "(985, 173, 40)\n",
            "(985,)\n",
            "10 class Accuracy:  tf.Tensor(0.8736842, shape=(), dtype=float32)\n",
            "BINARY Accuracy:  tf.Tensor(0.9442105, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}