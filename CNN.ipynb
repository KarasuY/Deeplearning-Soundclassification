{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKGmJuLeUQ7d",
        "outputId": "b7cb15f4-257d-434d-f350-3c4d0c0450e7"
      },
      "source": [
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "import pickle \n",
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "from google.colab import drive #Can ignore if done locally\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it-lJvapclBF"
      },
      "source": [
        "def preprocess(filepath): \n",
        "  \"\"\"\n",
        "    1. Unpickle file\n",
        "    2. Separate \n",
        "    3. One-hot encode labels \n",
        "\n",
        "    :inputs: \n",
        "    filepath: filepath to the pickle file in Drive \n",
        "\n",
        "    :returns: \n",
        "    (inputs, labels, folders)\n",
        "  \"\"\"\n",
        "  \n",
        "  with open(filepath, 'rb') as fo:\n",
        "    pickle_output = pickle.load(fo, encoding='bytes')\n",
        "  \n",
        "  inputs = [row[0] for row in pickle_output]\n",
        "  inputs = [inputs[i][:173] for i in range(len(inputs))]\n",
        "  inputs = np.array(inputs)\n",
        "  labels = np.array(pickle_output)[:, 1]\n",
        "  folders = np.array(pickle_output)[:, 2]\n",
        "\n",
        "  return (inputs, labels, folders)\n",
        "\n",
        "def split(inputs, labels, folders, test_folder_idx):\n",
        "  \"\"\"\n",
        "    Split data into training and testing data \n",
        "\n",
        "    :inputs: \n",
        "    the outputs from preprocess \n",
        "    test_folder_idx: index of the folder that will be used for testing\n",
        "\n",
        "    :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  \"\"\"\n",
        "  test_indices = np.nonzero(folders == test_folder_idx)\n",
        "  train_indices = np.nonzero(folders != test_folder_idx)\n",
        "\n",
        "  return (inputs[train_indices], labels[train_indices], inputs[test_indices], labels[test_indices])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOWn7o16VYN"
      },
      "source": [
        "def shuffle(inputs, labels, test_fraction):\n",
        "  '''\n",
        "  shuffle collection of all data, and split into testing and training, 15%:85%\n",
        "  this is for the method that requires the \n",
        "\n",
        "  :inputs: \n",
        "    the outputs from preprocess (inputs and labels)\n",
        "    test_fraction: percentage of inputs that will be used for testing\n",
        "  \n",
        "  :return: \n",
        "    one quadruple, (train_inputs, train_labels, test_inputs, test_labels)\n",
        "  '''\n",
        "  indices = np.arange(labels.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  inputs = np.take(inputs, indices, axis=0)\n",
        "  labels = np.take(labels, indices, axis=0)\n",
        "\n",
        "  num_test = int(test_fraction * labels.shape[0])\n",
        "  test_inputs = inputs[:num_test]\n",
        "  test_labels = labels[:num_test]\n",
        "  train_inputs = inputs[num_test:]\n",
        "  train_labels = labels[num_test:]\n",
        "\n",
        "  return (train_inputs, train_labels, test_inputs, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x8QpbkXT-dF"
      },
      "source": [
        "class CNNModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_batches):\n",
        "    super(CNNModel, self).__init__()\n",
        "\n",
        "    self.dropout_rate = 0.25\n",
        "    self.batch_size = num_batches\n",
        "\n",
        "    #adam optimizer\n",
        "    self.optimizer = tf.keras.optimizers.Adam(lr = 1e-4)\n",
        "\n",
        "    #initialize layers\n",
        "    self.dense1 = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=4, kernel_size=(5,5), strides=(4,1), activation='relu')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,1), activation='relu')\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(2,2), strides=(2,1), activation='relu')\n",
        "   \n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters=300, kernel_size=(2,2), strides=(1,1), activation='relu')\n",
        "\n",
        "    self.maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "    self.maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 1), strides=(2,1))\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "      call function takes in the inputs for the batch\n",
        "      and returns 10 class probabilities \n",
        "    \"\"\"\n",
        "    inputs = tf.expand_dims(inputs, axis=3)\n",
        "\n",
        "    convlayer1 = self.conv1(inputs)\n",
        "    convlayer2 = self.conv2(convlayer1)\n",
        "    maxpool1 = self.maxpool1(convlayer2)\n",
        "\n",
        "    drop1 = self.dropout1(maxpool1)\n",
        "    convlayer3 = self.conv3(drop1)\n",
        "    maxpool2 = self.maxpool2(convlayer3)\n",
        "\n",
        "    drop2 = self.dropout2(maxpool2)\n",
        "    convlayer4 = self.conv4(drop2)\n",
        "    drop3 = self.dropout3(convlayer4)\n",
        "    drop3 = tf.reshape(drop3, [self.batch_size, -1])\n",
        "    dense = self.dense1(drop3)\n",
        "\n",
        "    return dense\n",
        "\n",
        "  def loss(self, logits, labels):\n",
        "    \"\"\"\n",
        "    Calculates average cross entropy loss of the prediction\n",
        "\n",
        "    :param logits: a matrix of logits as a tensor\n",
        "    :param labels: matrix of labels containing the labels\n",
        "    :return: the loss of the model as a tensor of size 1\n",
        "\n",
        "    As cited in the paper, Table IV Experimental Results, authors used Categorical\n",
        "    cross entropy loss to measure their CNN+LSTM models\n",
        "    \"\"\"\n",
        "    losses = tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=False)\n",
        "    return tf.reduce_mean(losses)\n",
        "\n",
        "  def accuracy(self, logits, labels):\n",
        "\n",
        "    correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(labels,1))\n",
        "    return tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "    \n",
        "  def accuracy_2(self, probabilities, labels): \n",
        "    \"\"\"\n",
        "      returns the TOTAL NUMBER correct for our binary classification \n",
        "      (dangerous versus non-dangerous sounds)\n",
        "    \"\"\"\n",
        "    # dangerous labels include car_horn, dog_bark, drilling, gun_shot, jackhammer, siren\n",
        "    DANGEROUS_LABELS = [1, 3, 4, 6, 7, 8]  \n",
        "\n",
        "    # get the correct classification\n",
        "    classified = tf.argmax(probabilities, 1)\n",
        "    # for each classification, classify it as dangerous or not dangerous \n",
        "    classified_binary = tf.map_fn(lambda x: x in DANGEROUS_LABELS, classified)\n",
        "    \n",
        "    # get the label, and for each label classify it as dangerous or not dangerous\n",
        "    labels_classes = tf.argmax(labels, 1)\n",
        "    labels_binary = tf.map_fn(lambda x: x in DANGEROUS_LABELS, labels_classes)\n",
        "\n",
        "    # count the overlap and return the number correct in the given batch \n",
        "    correct_predictions = tf.equal(classified_binary, labels_binary)\n",
        "    return tf.reduce_sum(tf.cast(correct_predictions, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KCageXMUgCz"
      },
      "source": [
        "def train(model, train_inputs, train_labels):\n",
        "  \"\"\"\n",
        "\tRuns through one epoch - all training examples.\n",
        "\n",
        "  :param model: the initialized model to use for forward and backward pass\n",
        "\t:param train_inputs: input train data (all data for training) \n",
        "\t:param train_labels: labels train data (all data for training)\n",
        "\t:returns: None\n",
        "\t\"\"\"\n",
        "  for i in range(len(train_inputs) // model.batch_size):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = train_inputs[start : start + model.batch_size]\n",
        "      labels = train_labels[start : start + model.batch_size]\n",
        "\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # forward pass \n",
        "          logits = model.call(inputs)\n",
        "          loss = model.loss(logits, labels)\n",
        "                  \n",
        "      # backprop \n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "def test(model, test_inputs, test_labels, binary=False): \n",
        "  \"\"\"\n",
        "    returns the TOTAL accuracy for a single FOLDER \n",
        "  \"\"\"\n",
        "  num_batches = len(test_inputs) // model.batch_size\n",
        "  total_right = 0\n",
        "\n",
        "  for i in range(num_batches):\n",
        "      # getting the proper batch \n",
        "      start = i * model.batch_size \n",
        "      inputs = test_inputs[start : start + model.batch_size]\n",
        "      labels = test_labels[start : start + model.batch_size]\n",
        "\n",
        "      # calling the model to get our probabilities \n",
        "      probabilities = model.call(inputs)\n",
        "      if binary: \n",
        "        total_right += model.accuracy_2(probabilities, labels)\n",
        "      else: \n",
        "        total_right += model.accuracy(probabilities, labels)\n",
        "  \n",
        "  return total_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_weaLzcuHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c4d168-6398-4772-f4b6-b39230aaa295"
      },
      "source": [
        "\"\"\"\n",
        "  Our 'main' function that runs the model and computes\n",
        "  10 fold cross validation and binary accuracy \n",
        "\"\"\"\n",
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 50, 250\n",
        "  \n",
        "accuracy, binary_accuracy = 0, 0 \n",
        "total_tested = 0 \n",
        "for i in range(10): \n",
        "  print(\"Split/test folder: \", i + 1) \n",
        "  model = CNNModel(batch_size)\n",
        "  tr_in, tr_lb, te_in, te_lb = split(inputs, labels, folders, i + 1)\n",
        "  tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "  te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "  tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "  te_lb = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "\n",
        "  for _ in range(num_epochs): \n",
        "    train(model, tr_in, tr_lb)\n",
        "\n",
        "  per_fold_acc, binary_fold_acc = test(model, te_in, te_lb), test(model, te_in, te_lb, True)\n",
        "  accuracy += per_fold_acc\n",
        "  binary_accuracy += binary_fold_acc\n",
        "     \n",
        "  per_fold_tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "  total_tested += per_fold_tested\n",
        "  print('per-fold acc (10-class): ' + str(per_fold_acc / per_fold_tested))\n",
        "  print('per-fold acc (BINARY):   ' + str(binary_fold_acc / per_fold_tested))\n",
        "  \n",
        "print(\"Total Average Accuracy (10-class): \", accuracy / total_tested)\n",
        "print(\"Total Average Accuracy (BINARY):   \", binary_accuracy / total_tested)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split/test folder:  1\n",
            "per-fold acc (10-class): tf.Tensor(0.5476923, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.7953846, shape=(), dtype=float32)\n",
            "Split/test folder:  2\n",
            "per-fold acc (10-class): tf.Tensor(0.5323077, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.86153847, shape=(), dtype=float32)\n",
            "Split/test folder:  3\n",
            "per-fold acc (10-class): tf.Tensor(0.56285715, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.78, shape=(), dtype=float32)\n",
            "Split/test folder:  4\n",
            "per-fold acc (10-class): tf.Tensor(0.6028572, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.79285717, shape=(), dtype=float32)\n",
            "Split/test folder:  5\n",
            "per-fold acc (10-class): tf.Tensor(0.6142857, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.81714284, shape=(), dtype=float32)\n",
            "Split/test folder:  6\n",
            "per-fold acc (10-class): tf.Tensor(0.50166667, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.7916667, shape=(), dtype=float32)\n",
            "Split/test folder:  7\n",
            "per-fold acc (10-class): tf.Tensor(0.725, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.855, shape=(), dtype=float32)\n",
            "Split/test folder:  8\n",
            "per-fold acc (10-class): tf.Tensor(0.56545454, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.8218182, shape=(), dtype=float32)\n",
            "Split/test folder:  9\n",
            "per-fold acc (10-class): tf.Tensor(0.6533333, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.835, shape=(), dtype=float32)\n",
            "Split/test folder:  10\n",
            "per-fold acc (10-class): tf.Tensor(0.64666665, shape=(), dtype=float32)\n",
            "per-fold acc (BINARY):   tf.Tensor(0.8066667, shape=(), dtype=float32)\n",
            "Total Average Accuracy (10-class):  tf.Tensor(0.5944882, shape=(), dtype=float32)\n",
            "Total Average Accuracy (BINARY):    tf.Tensor(0.81496066, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G34do04U6kFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d832b1-0f37-4a14-cdff-9401c9cc8260"
      },
      "source": [
        "'''\n",
        "Shuffled version: extracted 15% for testing (follows paper) - Inflated scores\n",
        "'''\n",
        "pickled_path = '/content/gdrive/Shared drives/CS1470-Final/mfccs.pkl'\n",
        "inputs, labels, folders = preprocess(pickled_path)\n",
        "\n",
        "batch_size, num_epochs = 100, 250\n",
        "\n",
        "model = CNNModel(batch_size)\n",
        "print(np.shape(inputs))\n",
        "tr_in, tr_lb, te_in, te_lb = shuffle(inputs, labels, 0.15)\n",
        "\n",
        "tr_in = tf.convert_to_tensor(tr_in, dtype=tf.float32)\n",
        "te_in = tf.convert_to_tensor(te_in, dtype=tf.float32)\n",
        "tr_lb = tf.one_hot(tr_lb, 10, dtype=tf.int64)\n",
        "te_lb = tf.one_hot(te_lb, 10, dtype=tf.int64)\n",
        "\n",
        "for _ in range(num_epochs): \n",
        "  train(model, tr_in, tr_lb)\n",
        "\n",
        "acc, binary_acc = test(model, te_in, te_lb), test(model, te_in, te_lb, True)\n",
        "tested = (len(te_lb) - (len(te_lb) % batch_size))\n",
        "print(\"10 class Accuracy: \", acc / tested)\n",
        "print(\"BINARY Accuracy: \", binary_acc / tested)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6573, 173, 40)\n",
            "10 class Accuracy:  tf.Tensor(0.9022222, shape=(), dtype=float32)\n",
            "BINARY Accuracy:  tf.Tensor(0.9533333, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}